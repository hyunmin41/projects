{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cbef4258b635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~pdata.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"~pdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[0:118]\n",
    "df = df[['날짜', '정책', '정책지수', '누적정책지수', '서울', '서울 비율', '용산구', '광진구', '서대문구',\n",
    "       '영등포구', '서초구', '소비자물가지수', '한국은행기준금리', '원달러환율', '가계대출', '서울시부동산심리지수',\n",
    "       '서울실거래지수', '도심권실거래지수', '동북권실거래지수', '서북권실거래지수', '서남권실거래지수', '동남권실거래지수']]\n",
    "df.columns = [\"date\",\"p1\", \"p2\",\"p3\",\"Seoul\",\"Seoul_ratio\",\"Yongsan-gu\",\"Gwangjin-gu\",\"Seodaemun-gu\",\n",
    "              \"Yeongdeungpo-gu\",\"Seocho-gu\", \"CPI\",\"BR\",\"ER\",\"HL\", \"CSI\",\n",
    "              \"SP\", \"MP\", \"ENP\", \"WNP\", \"WSP\", \"ESP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"ER\"]=df[\"ER\"].str.replace(',', '').astype('float')\n",
    "#df[\"HL\"]=df[\"HL\"].str.replace(',', '').astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"date\",\"p1\",\"p2\",\"p3\",\"Seoul\",\"Yongsan-gu\",\"Gwangjin-gu\",\"Seodaemun-gu\",\n",
    "              \"Yeongdeungpo-gu\",\"Seocho-gu\",\"CPI\",\"BR\",\"ER\",\"HL\", \"CSI\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"Yongsan-gu\",\"Gwangjin-gu\",\"Seodaemun-gu\",\n",
    "              \"Yeongdeungpo-gu\",\"Seocho-gu\",\"p2\",\"CPI\",\"BR\",\"ER\",\"HL\", \"CSI\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split feature and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[[\"p2\",\"CPI\",\"BR\",\"ER\",\"HL\",\"CSI\"]]\n",
    "df2 = df[[\"Yongsan-gu\",\"p2\",\"CPI\",\"BR\",\"ER\",\"HL\",\"CSI\"]]\n",
    "df3 = df[[\"Gwangjin-gu\", \"p2\",\"CPI\",\"BR\",\"ER\",\"HL\",\"CSI\"]]\n",
    "df4 = df[[\"Seodaemun-gu\",\"p2\",\"CPI\",\"BR\",\"ER\",\"HL\",\"CSI\"]]\n",
    "df5 = df[[\"Yeongdeungpo-gu\", \"p2\",\"CPI\",\"BR\",\"ER\",\"HL\",\"CSI\"]]\n",
    "df6 = df[[\"Seocho-gu\", \"p2\", \"CPI\",\"BR\",\"ER\",\"HL\",\"CSI\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yongsan-gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "label=df2\n",
    "\n",
    "#feature.sort_index(ascending=False).reset_index(drop=True)\n",
    "#label.sort_index(ascending=False).reset_index(drop=True)\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "label_scaler = MinMaxScaler()\n",
    "\n",
    "feature = feature_scaler.fit_transform(df1)\n",
    "feature = pd.DataFrame(feature)\n",
    "\n",
    "label = label_scaler.fit_transform(label)\n",
    "label = pd.DataFrame(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
    "\n",
    "kfold=KFold(10)\n",
    "cvscores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model_path = 'model'\n",
    "filename = os.path.join(model_path, 'tmp_checkpoint.h5')\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Dense,Flatten,LSTM,Dropout,Conv1D, MaxPooling1D, TimeDistributed\n",
    "\n",
    "for train, test in kfold.split(feature,label):\n",
    "    \n",
    "    train_feature, test_feature = feature[0:106], feature[93:106]\n",
    "    train_label, test_label = label[0:106], label[93:106]\n",
    "    pred_feature, pred_label = feature[105:118], label[105:118]\n",
    "\n",
    "    train_feature, train_label = make_dataset(train_feature,train_label, 6)\n",
    "    test_feature, test_label = make_dataset(test_feature, test_label, 6)\n",
    "    pred_feature, pred_label = make_dataset(pred_feature, pred_label, 6)\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "    \n",
    "    subsequences = 2\n",
    "    timesteps = x_train.shape[1]//subsequences\n",
    "    X_train_series_sub = x_train.reshape((x_train.shape[0], subsequences, timesteps, 6)) \n",
    "    X_valid_series_sub = x_valid.reshape((x_valid.shape[0], subsequences, timesteps, 6))\n",
    "    test_feature_series_sub = test_feature.reshape((test_feature.shape[0], subsequences, timesteps, 6))\n",
    "    pred_feature_series_sub = pred_feature.reshape((pred_feature.shape[0], subsequences, timesteps, 6))\n",
    "    \n",
    "    output_size=6 \n",
    "    neurons=100 \n",
    "    loss=\"mean_squared_error\" \n",
    "    \n",
    "    model_cnn_lstm = Sequential()\n",
    "    model_cnn_lstm.add(TimeDistributed(Conv1D(filters=128, kernel_size=2, \n",
    "                                          activation='relu'),\n",
    "                                   input_shape=(None, X_train_series_sub.shape[2],\n",
    "                                                X_train_series_sub.shape[3])))\n",
    "    model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model_cnn_lstm.add(TimeDistributed(Dropout((0.5))))\n",
    "    model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "    model_cnn_lstm.add(LSTM(100, activation='relu'))\n",
    "    model_cnn_lstm.add(Dropout(0.25))\n",
    "    model_cnn_lstm.add(Dense(150))\n",
    "    model_cnn_lstm.add(Dense(1))\n",
    "    model_cnn_lstm.compile(loss=loss, optimizer=\"adam\", metrics=[\"mse\"])\n",
    "    model_cnn_lstm.fit(X_train_series_sub, y_train,\n",
    "                    epochs=50, \n",
    "                    verbose=1, \n",
    "                    validation_data=(X_valid_series_sub, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "    scores = model_cnn_lstm.evaluate(test_feature_series_sub, test_label, verbose=0)\n",
    "    cvscores = (scores * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model_cnn_lstm.predict(test_feature_series_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def RMSE(y_test, y_predict):\n",
    "    return np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "print('RMSE : ', RMSE(test_label, test_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = label_scaler.inverse_transform(test_pred)\n",
    "test_label = label_scaler.inverse_transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_cnn_lstm.predict(pred_feature_series_sub)\n",
    "true = pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = label_scaler.inverse_transform(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = label2.tail(6)\n",
    "true = np.array(true)\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true, label = 'actual')\n",
    "plt.plot(pred, label = 'prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=label2\n",
    "#b=pd.DataFrame(pred, columns = [\"Yongsan-gu\"])\n",
    "#mod_df = a.append(b.loc[:], ignore_index=True)\n",
    "#mod_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gwangjin-gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "label=label3\n",
    "\n",
    "#feature.sort_index(ascending=False).reset_index(drop=True)\n",
    "#label.sort_index(ascending=False).reset_index(drop=True)\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "label_scaler = MinMaxScaler()\n",
    "\n",
    "feature = feature_scaler.fit_transform(feature)\n",
    "feature = pd.DataFrame(feature)\n",
    "\n",
    "label = label_scaler.fit_transform(label)\n",
    "label = pd.DataFrame(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Dense,Flatten,LSTM,Dropout,Conv1D, MaxPooling1D, TimeDistributed\n",
    "\n",
    "for train, test in kfold.split(feature,label):\n",
    "    \n",
    "    train_feature, test_feature = feature[0:106], feature[93:106]\n",
    "    train_label, test_label = label[0:106], label[93:106]\n",
    "    pred_feature, pred_label = feature[105:118], label[105:118]\n",
    "\n",
    "    train_feature, train_label = make_dataset(train_feature,train_label, 6)\n",
    "    test_feature, test_label = make_dataset(test_feature, test_label, 6)\n",
    "    pred_feature, pred_label = make_dataset(pred_feature, pred_label, 6)\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "    \n",
    "    subsequences = 2\n",
    "    timesteps = x_train.shape[1]//subsequences\n",
    "    X_train_series_sub = x_train.reshape((x_train.shape[0], subsequences, timesteps, 6)) \n",
    "    X_valid_series_sub = x_valid.reshape((x_valid.shape[0], subsequences, timesteps, 6))\n",
    "    test_feature_series_sub = test_feature.reshape((test_feature.shape[0], subsequences, timesteps, 6))\n",
    "    pred_feature_series_sub = pred_feature.reshape((pred_feature.shape[0], subsequences, timesteps, 6))\n",
    "    \n",
    "    output_size=6 \n",
    "    neurons=100 \n",
    "    loss=\"mean_squared_error\" \n",
    "    \n",
    "    model_cnn_lstm = Sequential()\n",
    "    model_cnn_lstm.add(TimeDistributed(Conv1D(filters=128, kernel_size=2, \n",
    "                                          activation='relu'),\n",
    "                                   input_shape=(None, X_train_series_sub.shape[2],\n",
    "                                                X_train_series_sub.shape[3])))\n",
    "    model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model_cnn_lstm.add(TimeDistributed(Dropout((0.5))))\n",
    "    model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "    model_cnn_lstm.add(LSTM(100, activation='relu'))\n",
    "    model_cnn_lstm.add(Dropout(0.25))\n",
    "    model_cnn_lstm.add(Dense(150))\n",
    "    model_cnn_lstm.add(Dense(1))\n",
    "    model_cnn_lstm.compile(loss=loss, optimizer=\"adam\", metrics=[\"mse\"])\n",
    "    model_cnn_lstm.fit(X_train_series_sub, y_train,\n",
    "                    epochs=50, \n",
    "                    verbose=1, \n",
    "                    validation_data=(X_valid_series_sub, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "    scores = model_cnn_lstm.evaluate(test_feature_series_sub, test_label, verbose=0)\n",
    "    cvscores = (scores * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model_cnn_lstm.predict(test_feature_series_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(test_label, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = label_scaler.inverse_transform(test_pred)\n",
    "test_label = label_scaler.inverse_transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_cnn_lstm.predict(pred_feature_series_sub)\n",
    "true = pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = label_scaler.inverse_transform(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = label3.tail(6)\n",
    "true = np.array(true)\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true, label = 'actual')\n",
    "plt.plot(pred, label = 'prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seodaemun-gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "label=label4\n",
    "\n",
    "#feature.sort_index(ascending=False).reset_index(drop=True)\n",
    "#label.sort_index(ascending=False).reset_index(drop=True)\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "label_scaler = MinMaxScaler()\n",
    "\n",
    "feature = feature_scaler.fit_transform(feature)\n",
    "feature = pd.DataFrame(feature)\n",
    "\n",
    "label = label_scaler.fit_transform(label)\n",
    "label = pd.DataFrame(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Dense,Flatten,LSTM,Dropout,Conv1D, MaxPooling1D, TimeDistributed\n",
    "\n",
    "for train, test in kfold.split(feature,label):\n",
    "    \n",
    "    train_feature, test_feature = feature[0:106], feature[93:106]\n",
    "    train_label, test_label = label[0:106], label[93:106]\n",
    "    pred_feature, pred_label = feature[105:118], label[105:118]\n",
    "\n",
    "    train_feature, train_label = make_dataset(train_feature,train_label, 6)\n",
    "    test_feature, test_label = make_dataset(test_feature, test_label, 6)\n",
    "    pred_feature, pred_label = make_dataset(pred_feature, pred_label, 6)\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "    \n",
    "    subsequences = 2\n",
    "    timesteps = x_train.shape[1]//subsequences\n",
    "    X_train_series_sub = x_train.reshape((x_train.shape[0], subsequences, timesteps, 6)) \n",
    "    X_valid_series_sub = x_valid.reshape((x_valid.shape[0], subsequences, timesteps, 6))\n",
    "    test_feature_series_sub = test_feature.reshape((test_feature.shape[0], subsequences, timesteps, 6))\n",
    "    pred_feature_series_sub = pred_feature.reshape((pred_feature.shape[0], subsequences, timesteps, 6))\n",
    "    \n",
    "    output_size=6 \n",
    "    neurons=100 \n",
    "    loss=\"mean_squared_error\" \n",
    "    \n",
    "    model_cnn_lstm = Sequential()\n",
    "    model_cnn_lstm.add(TimeDistributed(Conv1D(filters=128, kernel_size=2, \n",
    "                                          activation='relu'),\n",
    "                                   input_shape=(None, X_train_series_sub.shape[2],\n",
    "                                                X_train_series_sub.shape[3])))\n",
    "    model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model_cnn_lstm.add(TimeDistributed(Dropout((0.5))))\n",
    "    model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "    model_cnn_lstm.add(LSTM(100, activation='relu'))\n",
    "    model_cnn_lstm.add(Dropout(0.25))\n",
    "    model_cnn_lstm.add(Dense(150))\n",
    "    model_cnn_lstm.add(Dense(1))\n",
    "    model_cnn_lstm.compile(loss=loss, optimizer=\"adam\", metrics=[\"mse\"])\n",
    "    model_cnn_lstm.fit(X_train_series_sub, y_train,\n",
    "                    epochs=50, \n",
    "                    verbose=1, \n",
    "                    validation_data=(X_valid_series_sub, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "    scores = model_cnn_lstm.evaluate(test_feature_series_sub, test_label, verbose=0)\n",
    "    cvscores = (scores * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model_cnn_lstm.predict(test_feature_series_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(test_label, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = label_scaler.inverse_transform(test_pred)\n",
    "test_label = label_scaler.inverse_transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_cnn_lstm.predict(pred_feature_series_sub)\n",
    "true = pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = label_scaler.inverse_transform(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = label4.tail(6)\n",
    "true = np.array(true)\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true, label = 'actual')\n",
    "plt.plot(pred, label = 'prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yeongdeungpo-gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "label=label5\n",
    "\n",
    "#feature.sort_index(ascending=False).reset_index(drop=True)\n",
    "#label.sort_index(ascending=False).reset_index(drop=True)\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "label_scaler = MinMaxScaler()\n",
    "\n",
    "feature = feature_scaler.fit_transform(feature)\n",
    "feature = pd.DataFrame(feature)\n",
    "\n",
    "label = label_scaler.fit_transform(label)\n",
    "label = pd.DataFrame(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Dense,Flatten,LSTM,Dropout,Conv1D, MaxPooling1D, TimeDistributed\n",
    "\n",
    "for train, test in kfold.split(feature,label):\n",
    "    \n",
    "    train_feature, test_feature = feature[0:106], feature[93:106]\n",
    "    train_label, test_label = label[0:106], label[93:106]\n",
    "    pred_feature, pred_label = feature[105:118], label[105:118]\n",
    "\n",
    "    train_feature, train_label = make_dataset(train_feature,train_label, 6)\n",
    "    test_feature, test_label = make_dataset(test_feature, test_label, 6)\n",
    "    pred_feature, pred_label = make_dataset(pred_feature, pred_label, 6)\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "    \n",
    "    subsequences = 2\n",
    "    timesteps = x_train.shape[1]//subsequences\n",
    "    X_train_series_sub = x_train.reshape((x_train.shape[0], subsequences, timesteps, 6)) \n",
    "    X_valid_series_sub = x_valid.reshape((x_valid.shape[0], subsequences, timesteps, 6))\n",
    "    test_feature_series_sub = test_feature.reshape((test_feature.shape[0], subsequences, timesteps, 6))\n",
    "    pred_feature_series_sub = pred_feature.reshape((pred_feature.shape[0], subsequences, timesteps, 6))\n",
    "    \n",
    "    output_size=6 \n",
    "    neurons=100 \n",
    "    loss=\"mean_squared_error\" \n",
    "    \n",
    "    model_cnn_lstm = Sequential()\n",
    "    model_cnn_lstm.add(TimeDistributed(Conv1D(filters=128, kernel_size=2, \n",
    "                                          activation='relu'),\n",
    "                                   input_shape=(None, X_train_series_sub.shape[2],\n",
    "                                                X_train_series_sub.shape[3])))\n",
    "    model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model_cnn_lstm.add(TimeDistributed(Dropout((0.5))))\n",
    "    model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "    model_cnn_lstm.add(LSTM(100, activation='relu'))\n",
    "    model_cnn_lstm.add(Dropout(0.25))\n",
    "    model_cnn_lstm.add(Dense(150))\n",
    "    model_cnn_lstm.add(Dense(1))\n",
    "    model_cnn_lstm.compile(loss=loss, optimizer=\"adam\", metrics=[\"mse\"])\n",
    "    model_cnn_lstm.fit(X_train_series_sub, y_train,\n",
    "                    epochs=50, \n",
    "                    verbose=1, \n",
    "                    validation_data=(X_valid_series_sub, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "    scores = model_cnn_lstm.evaluate(test_feature_series_sub, test_label, verbose=0)\n",
    "    cvscores = (scores * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model_cnn_lstm.predict(test_feature_series_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(test_label, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = label_scaler.inverse_transform(test_pred)\n",
    "test_label = label_scaler.inverse_transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_cnn_lstm.predict(pred_feature_series_sub)\n",
    "true = pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = label_scaler.inverse_transform(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = label5.tail(6)\n",
    "true = np.array(true)\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true, label = 'actual')\n",
    "plt.plot(pred, label = 'prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seocho-gu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "label=label6\n",
    "\n",
    "#feature.sort_index(ascending=False).reset_index(drop=True)\n",
    "#label.sort_index(ascending=False).reset_index(drop=True)\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "label_scaler = MinMaxScaler()\n",
    "\n",
    "feature = feature_scaler.fit_transform(feature)\n",
    "feature = pd.DataFrame(feature)\n",
    "\n",
    "label = label_scaler.fit_transform(label)\n",
    "label = pd.DataFrame(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Dense,Flatten,LSTM,Dropout,Conv1D, MaxPooling1D, TimeDistributed\n",
    "\n",
    "for train, test in kfold.split(feature,label):\n",
    "    \n",
    "    train_feature, test_feature = feature[0:106], feature[93:106]\n",
    "    train_label, test_label = label[0:106], label[93:106]\n",
    "    pred_feature, pred_label = feature[105:118], label[105:118]\n",
    "\n",
    "    train_feature, train_label = make_dataset(train_feature,train_label, 6)\n",
    "    test_feature, test_label = make_dataset(test_feature, test_label, 6)\n",
    "    pred_feature, pred_label = make_dataset(pred_feature, pred_label, 6)\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "    \n",
    "    subsequences = 2\n",
    "    timesteps = x_train.shape[1]//subsequences\n",
    "    X_train_series_sub = x_train.reshape((x_train.shape[0], subsequences, timesteps, 6)) \n",
    "    X_valid_series_sub = x_valid.reshape((x_valid.shape[0], subsequences, timesteps, 6))\n",
    "    test_feature_series_sub = test_feature.reshape((test_feature.shape[0], subsequences, timesteps, 6))\n",
    "    pred_feature_series_sub = pred_feature.reshape((pred_feature.shape[0], subsequences, timesteps, 6))\n",
    "    \n",
    "    output_size=6 \n",
    "    neurons=100 \n",
    "    loss=\"mean_squared_error\" \n",
    "    \n",
    "    model_cnn_lstm = Sequential()\n",
    "    model_cnn_lstm.add(TimeDistributed(Conv1D(filters=128, kernel_size=2, \n",
    "                                          activation='relu'),\n",
    "                                   input_shape=(None, X_train_series_sub.shape[2],\n",
    "                                                X_train_series_sub.shape[3])))\n",
    "    model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model_cnn_lstm.add(TimeDistributed(Dropout((0.5))))\n",
    "    model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "    model_cnn_lstm.add(LSTM(100, activation='relu'))\n",
    "    model_cnn_lstm.add(Dropout(0.25))\n",
    "    model_cnn_lstm.add(Dense(150))\n",
    "    model_cnn_lstm.add(Dense(1))\n",
    "    model_cnn_lstm.compile(loss=loss, optimizer=\"adam\", metrics=[\"mse\"])\n",
    "    model_cnn_lstm.fit(X_train_series_sub, y_train,\n",
    "                    epochs=50, \n",
    "                    verbose=1, \n",
    "                    validation_data=(X_valid_series_sub, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "    scores = model_cnn_lstm.evaluate(test_feature_series_sub, test_label, verbose=0)\n",
    "    cvscores = (scores * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model_cnn_lstm.predict(test_feature_series_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(test_label, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = label_scaler.inverse_transform(test_pred)\n",
    "test_label = label_scaler.inverse_transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_cnn_lstm.predict(pred_feature_series_sub)\n",
    "true = pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = label_scaler.inverse_transform(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = label6.tail(6)\n",
    "true = np.array(true)\n",
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true, label = 'actual')\n",
    "plt.plot(pred, label = 'prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
